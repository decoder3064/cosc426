{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367bce49-2464-4b8f-8c33-e2a66ed47e59",
   "metadata": {},
   "source": [
    "# Lab 1: NLPScholar practice\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer the questions in `Lab1.md`. For answers where you are looking at data files generated by the `evaluate` or `analyze` modes (which is almost all of them), you should load the data into the ipynb notebook and proceed from there. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7fdbf",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "\n",
    "Submit the following files to gradescope:\n",
    "\n",
    "* `Lab1.ipynb` with answers to all of the questions. **Make sure that when you submit the file, the outputs in the cells are visible on gradescope.** \n",
    "* All config files you created. Create a separate file for each config setting you use. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759fb07",
   "metadata": {},
   "source": [
    "## Part 1: Minimal Pair\n",
    "\n",
    "In this part you will be working with three models: \n",
    "* `gpt2`\n",
    "* `distilbert/distilgpt2`\n",
    "* `distilbert-base-uncased`\n",
    "\n",
    "Answer the following questions for all the three models: \n",
    "\n",
    "1. What is the difference in surprisal between expected and unexpected when the verb lemma is `LIKE` when we consider the words specified in the ROI column?\n",
    "\n",
    "2. Is the difference in surprisal between expected and unexpected greater for singular verbs compared to plural verbs when we consider the words specified in the ROI column? \n",
    "\n",
    "3. Is the answer to question 2 different if you look at the surprisal of the entire sentence? \n",
    "\n",
    "4. What is the mean probability of the expected (i.e., grammatical) sentences in `agreement.tsv` (over the entire sentence)?\n",
    "\n",
    "5. What is the mean probability of the word `chameleons` in the expected sentences when you consider the period vs. not (i.e., `chameleons` vs. `chameleons.`)\n",
    "\n",
    "After you are done with this, answer the following question: \n",
    "\n",
    "6. What is bad about `minimal_pairs_bad.tsv`? What happens when you run `evaluate` on this data file? What happens when you run `analyze` on this data file? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7236bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ad76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './results/agreement.tsv'\n",
    "df = pd.read_csv(filepath,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347017bc",
   "metadata": {},
   "source": [
    "1. What is the difference in surprisal between expected and unexpected when the verb lemma is `LIKE` when we consider the words specified in the ROI column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a20a468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>-2.278639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>-1.516554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>-1.550564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model      diff\n",
       "1  distilbert-base-uncased -2.278639\n",
       "3    distilbert/distilgpt2 -1.516554\n",
       "5                     gpt2 -1.550564"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lemma'] == 'LIKE'][['model', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25ca2d",
   "metadata": {},
   "source": [
    "2. Is the difference in surprisal between expected and unexpected greater for singular verbs compared to plural verbs when we consider the words specified in the ROI column? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "237ea578",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './results/agreement.tsv'\n",
    "df = pd.read_csv(filepath,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c711336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>number</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>plu</td>\n",
       "      <td>-4.133038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>sing</td>\n",
       "      <td>-2.329132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>plu</td>\n",
       "      <td>-3.141551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>sing</td>\n",
       "      <td>-1.433033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>plu</td>\n",
       "      <td>-3.184353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>sing</td>\n",
       "      <td>-1.607013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model number      diff\n",
       "0  distilbert-base-uncased    plu -4.133038\n",
       "1  distilbert-base-uncased   sing -2.329132\n",
       "2    distilbert/distilgpt2    plu -3.141551\n",
       "3    distilbert/distilgpt2   sing -1.433033\n",
       "4                     gpt2    plu -3.184353\n",
       "5                     gpt2   sing -1.607013"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['model', 'number', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f7db3",
   "metadata": {},
   "source": [
    "As the table shows, the difference in surprisal between expected and unexpected outcomes is **greater for plural** verbs across all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2978a",
   "metadata": {},
   "source": [
    "3. Is the answer to question 2 different if you look at the surprisal of the entire sentence? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './results/agreement.tsv'\n",
    "df = pd.read_csv(filepath,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a631e4",
   "metadata": {},
   "source": [
    "4. What is the mean probability of the expected (i.e., grammatical) sentences in `agreement.tsv` (over the entire sentence)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd33ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e07c8a",
   "metadata": {},
   "source": [
    "5. What is the mean probability of the word `chameleons` in the expected sentences when you consider the period vs. not (i.e., `chameleons` vs. `chameleons.`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f333024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f934ac22",
   "metadata": {},
   "source": [
    "6. What is bad about `minimal_pairs_bad.tsv`? What happens when you run `evaluate` on this data file? What happens when you run `analyze` on this data file? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f1879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7577d77d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f8cb0-0c02-4e29-8b85-8408ce1a60d2",
   "metadata": {},
   "source": [
    "## Part 2: Token Classification\n",
    "\n",
    "In this part you will be working on a [Named Entity Recogniton](https://en.wikipedia.org/wiki/Named-entity_recognition) task with the following model: \n",
    "* `distilbert-base-uncased`\n",
    "\n",
    "Note: this model not been trained on the NER task. When you specify the model, you are just getting the architecture from Huggingface. So we should expect poor performance.  \n",
    "\n",
    "Answer the following questions: \n",
    "\n",
    "1. What is the overall accuracy of the model?\n",
    "2. What is the overall accuracy of the model if you ignore the `O` label (which indicates that there is no entity)\n",
    "3. What is the overall accuracy if you ignore punctuation and the `O` label?\n",
    "4. What is the accuracy of the `B-location` tag when you consider long vs. short sentences? \n",
    "\n",
    "After you are done with this, answer the following question: \n",
    "\n",
    "5. Here is a model that has been trained on NER task: [bert-base-NER](https://huggingface.co/dslim/bert-base-NER/tree/main). **This model has been trained on a different set of labels than the model you were evaluating.** If you wanted to use this model instead, what would you need to change in your NLPScholar config file? (*Hint: It might be helpful to look at the model's `config.json`*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda6c4c-7882-43ca-b9b0-b26383820e6c",
   "metadata": {},
   "source": [
    "## Part 3: Text Classification\n",
    "\n",
    "In this part you will be working on a Sentiment Analysis task with the following models: \n",
    "* `siebert/sentiment-roberta-large-english` (which has been trained on sentiment analysis)\n",
    "* `roberta-large` which has not been trained (like in the TokenClassification example)\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Is the accuracy of trained model greater than the untrained model? \n",
    "2. Is the difference between the trained and untrained model greater for full vs. one-line reviews? \n",
    "3. Is the f1 score for reviews that are positive greater than f1 score for reviews that are negative when you consider all reviews and all models? \n",
    "4. Is the answer to 3 different if you consider just the ChatGPT generated reviews? \n",
    "5. What is the average probability of the predicted label for both the models? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac47fe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
