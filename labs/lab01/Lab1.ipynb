{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7236bb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bce49-2464-4b8f-8c33-e2a66ed47e59",
   "metadata": {},
   "source": [
    "# Lab 1: NLPScholar practice\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer the questions in `Lab1.md`. For answers where you are looking at data files generated by the `evaluate` or `analyze` modes (which is almost all of them), you should load the data into the ipynb notebook and proceed from there. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7fdbf",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "\n",
    "Submit the following files to gradescope:\n",
    "\n",
    "* `Lab1.ipynb` with answers to all of the questions. **Make sure that when you submit the file, the outputs in the cells are visible on gradescope.** \n",
    "* All config files you created. Create a separate file for each config setting you use. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759fb07",
   "metadata": {},
   "source": [
    "## Part 1: Minimal Pair\n",
    "\n",
    "In this part you will be working with three models: \n",
    "* `gpt2`\n",
    "* `distilbert/distilgpt2`\n",
    "* `distilbert-base-uncased`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347017bc",
   "metadata": {},
   "source": [
    "1. What is the difference in surprisal between expected and unexpected when the verb lemma is `LIKE` when we consider the words specified in the ROI column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4af41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `conditions` to lemma\n",
    "df = pd.read_csv('./results/agreement.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a20a468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>-2.278639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>-1.516554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>-1.550564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model      diff\n",
       "1  distilbert-base-uncased -2.278639\n",
       "3    distilbert/distilgpt2 -1.516554\n",
       "5                     gpt2 -1.550564"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['lemma'] == 'LIKE'][['model', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25ca2d",
   "metadata": {},
   "source": [
    "2. Is the difference in surprisal between expected and unexpected greater for singular verbs compared to plural verbs when we consider the words specified in the ROI column? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ea578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `conditions` to number\n",
    "df = pd.read_csv('./results/agreement.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c711336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>number</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>plu</td>\n",
       "      <td>-4.133038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>sing</td>\n",
       "      <td>-2.329132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>plu</td>\n",
       "      <td>-3.141551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>sing</td>\n",
       "      <td>-1.433033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>plu</td>\n",
       "      <td>-3.184353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>sing</td>\n",
       "      <td>-1.607013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model number      diff\n",
       "0  distilbert-base-uncased    plu -4.133038\n",
       "1  distilbert-base-uncased   sing -2.329132\n",
       "2    distilbert/distilgpt2    plu -3.141551\n",
       "3    distilbert/distilgpt2   sing -1.433033\n",
       "4                     gpt2    plu -3.184353\n",
       "5                     gpt2   sing -1.607013"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['model', 'number', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f7db3",
   "metadata": {},
   "source": [
    "As the table shows, the difference in surprisal between expected and unexpected is **greater for plural** verbs across all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2978a",
   "metadata": {},
   "source": [
    "3. Is the answer to question 2 different if you look at the surprisal of the entire sentence? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed ROI column in the dataset\n",
    "df = pd.read_csv('./results/agreement.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "481325ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>number</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>plu</td>\n",
       "      <td>-2.580857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>sing</td>\n",
       "      <td>-1.291044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>plu</td>\n",
       "      <td>-1.570776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>sing</td>\n",
       "      <td>-0.481562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>plu</td>\n",
       "      <td>-1.592177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>sing</td>\n",
       "      <td>-0.610408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model number      diff\n",
       "0  distilbert-base-uncased    plu -2.580857\n",
       "1  distilbert-base-uncased   sing -1.291044\n",
       "2    distilbert/distilgpt2    plu -1.570776\n",
       "3    distilbert/distilgpt2   sing -0.481562\n",
       "4                     gpt2    plu -1.592177\n",
       "5                     gpt2   sing -0.610408"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['model', 'number', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e084f",
   "metadata": {},
   "source": [
    "While the difference in surprisal between expected and unexpected decreases for both plural and singular verbs, it is still **greater for plural** verbs across all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a631e4",
   "metadata": {},
   "source": [
    "4. What is the mean probability of the expected (i.e., grammatical) sentences in `agreement.tsv` (over the entire sentence)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd33ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed conditions\n",
    "df = pd.read_csv('./results/agreement.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6bb60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>7.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert/distilgpt2</td>\n",
       "      <td>6.631824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>6.330092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  expected\n",
       "0  distilbert-base-uncased  7.662000\n",
       "1    distilbert/distilgpt2  6.631824\n",
       "2                     gpt2  6.330092"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['model', 'expected']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e07c8a",
   "metadata": {},
   "source": [
    "5. What is the mean probability of the word `chameleons` in the expected sentences when you consider the period vs. not (i.e., `chameleons` vs. `chameleons.`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f333024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left out as instructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934ac22",
   "metadata": {},
   "source": [
    "6. What is bad about `minimal_pairs_bad.tsv`? What happens when you run `evaluate` on this data file? What happens when you run `analyze` on this data file? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81109ab",
   "metadata": {},
   "source": [
    "In `minimal_pairs_bad.tsv`, the `name` column was renamed to `condition`. The file is also missing matching pair IDs for some of its row. When I run evaluate, it seems to work without any warnings or errors, but when I run analyze, it returns two warnings: `WARNING: Excluding pairs which did not have expected or unexpected comparisons: {8, 9, 10, 7}` and `WARNING: No valid condition columns entered`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577d77d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f8cb0-0c02-4e29-8b85-8408ce1a60d2",
   "metadata": {},
   "source": [
    "## Part 2: Token Classification\n",
    "\n",
    "In this part you will be working on a [Named Entity Recogniton](https://en.wikipedia.org/wiki/Named-entity_recognition) task with the following model: \n",
    "* `distilbert-base-uncased`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcc418",
   "metadata": {},
   "source": [
    "1. What is the overall accuracy of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52c995c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  accuracy\n",
       "0  distilbert-base-uncased  0.043478"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/ner_bycond.tsv', delimiter='\\t')\n",
    "df[['model', 'accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa50b0",
   "metadata": {},
   "source": [
    "2. What is the overall accuracy of the model if you ignore the `O` label (which indicates that there is no entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a814e648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>0.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  accuracy\n",
       "0  distilbert-base-uncased    0.1875"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/ner_bycond.tsv', delimiter='\\t')\n",
    "df[['model', 'accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7def059",
   "metadata": {},
   "source": [
    "3. What is the overall accuracy if you ignore punctuation and the `O` label?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aead321d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  accuracy\n",
       "0  distilbert-base-uncased    0.0625"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/ner_bycond.tsv', delimiter='\\t')\n",
    "df[['model', 'accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbdb7c",
   "metadata": {},
   "source": [
    "4. What is the accuracy of the `B-location` tag when you consider long vs. short sentences? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "905d4ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>condition</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>full</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>short</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model condition  accuracy\n",
       "0  distilbert-base-uncased      full  0.200000\n",
       "1  distilbert-base-uncased     short  0.333333"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/ner_bycond.tsv', delimiter='\\t')\n",
    "df[['model','condition', 'accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1779dff",
   "metadata": {},
   "source": [
    "5. Here is a model that has been trained on NER task: [bert-base-NER](https://huggingface.co/dslim/bert-base-NER/tree/main). **This model has been trained on a different set of labels than the model you were evaluating.** If you wanted to use this model instead, what would you need to change in your NLPScholar config file? (*Hint: It might be helpful to look at the model's `config.json`*)\n",
    "\n",
    "Models & id2label. I would change the model name to bert-base-NER and change the id2label according to the new labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda6c4c-7882-43ca-b9b0-b26383820e6c",
   "metadata": {},
   "source": [
    "## Part 3: Text Classification\n",
    "\n",
    "In this part you will be working on a Sentiment Analysis task with the following models: \n",
    "* `siebert/sentiment-roberta-large-english` (which has been trained on sentiment analysis)\n",
    "* `roberta-large` which has not been trained (like in the TokenClassification example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32efb4",
   "metadata": {},
   "source": [
    "1. Is the accuracy of trained model greater than the untrained model? \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29561442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-large</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  accuracy\n",
       "0  roberta-large       0.5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/sentiment.tsv', delimiter='\\t')\n",
    "df[['model', 'accuracy']] \n",
    "# I've observed some strange behavior. When I run both models from a single configuration file, \n",
    "# they produce the same accuracy. However, when I run them separately using two different configuration files, \n",
    "# they produce different results. \n",
    "# Suspiciously, the accuracy from the combined run is the average of the two individual model accuracies.\n",
    "\n",
    "# I suspect there's a bug in the analysis mode. \n",
    "# Since I'm aware that parts of NLPScholar are still in development, \n",
    "# I've decided to focus on configuring the YAML file correctly, even if the results are inaccurate.\n",
    "# Not sure if I'm just misconfiguring the file, but it is starnge that what worked for other exp's is not working for this one case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba3fdea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     model  accuracy\n",
       "0  siebert/sentiment-roberta-large-english     0.875"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/sentiment.tsv', delimiter='\\t')\n",
    "df[['model', 'accuracy']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84597227",
   "metadata": {},
   "source": [
    "2. Is the difference between the trained and untrained model greater for full vs. one-line reviews? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6c7f6",
   "metadata": {},
   "source": [
    "They are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f13db4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-large</td>\n",
       "      <td>Full</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-large</td>\n",
       "      <td>One-line</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model      type  accuracy\n",
       "0  roberta-large      Full       0.5\n",
       "1  roberta-large  One-line       0.5"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/sentiment.tsv', delimiter='\\t')\n",
    "df[['model', 'type', 'accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "574e0ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>Full</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>One-line</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     model      type  accuracy\n",
       "0  siebert/sentiment-roberta-large-english      Full     0.875\n",
       "1  siebert/sentiment-roberta-large-english  One-line     0.875"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/sentiment.tsv', delimiter='\\t')\n",
    "df[['model', 'type', 'accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21dc57",
   "metadata": {},
   "source": [
    "3. Is the f1 score for reviews that are positive greater than f1 score for reviews that are negative when you consider all reviews and all models? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6ac035c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>target</th>\n",
       "      <th>macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-large</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.805668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-large</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.805668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.805668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.805668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     model    target  macro-f1\n",
       "0                            roberta-large  Negative  0.805668\n",
       "1                            roberta-large  Positive  0.805668\n",
       "2  siebert/sentiment-roberta-large-english  Negative  0.805668\n",
       "3  siebert/sentiment-roberta-large-english  Positive  0.805668"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/sentiment.tsv', delimiter='\\t')\n",
    "df[['model', 'target', 'macro-f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac47fe",
   "metadata": {},
   "source": [
    "4. Is the answer to 3 different if you consider just the ChatGPT generated reviews? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e7b0df38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>target</th>\n",
       "      <th>macro-f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roberta-large</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.65368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roberta-large</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.65368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.65368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.65368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     model    target  macro-f1\n",
       "0                            roberta-large  Negative   0.65368\n",
       "2                            roberta-large  Positive   0.65368\n",
       "4  siebert/sentiment-roberta-large-english  Negative   0.65368\n",
       "6  siebert/sentiment-roberta-large-english  Positive   0.65368"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/sentiment.tsv', delimiter='\\t')\n",
    "df[df['source']=='ChatGPT'][['model', 'target', 'macro-f1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a5bc5",
   "metadata": {},
   "source": [
    "5. What is the average probability of the predicted label for both the models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9d528664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>model</th>\n",
       "      <th>micro-precision</th>\n",
       "      <th>micro-recall</th>\n",
       "      <th>micro-f1</th>\n",
       "      <th>macro-precision</th>\n",
       "      <th>macro-recall</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>roberta-large</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.718182</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.676113</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>siebert/sentiment-roberta-large-english</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.718182</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.676113</td>\n",
       "      <td>0.6875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                    model  micro-precision  \\\n",
       "0           0                            roberta-large           0.6875   \n",
       "1           1  siebert/sentiment-roberta-large-english           0.6875   \n",
       "\n",
       "   micro-recall  micro-f1  macro-precision  macro-recall  macro-f1  accuracy  \n",
       "0        0.6875    0.6875         0.718182        0.6875  0.676113    0.6875  \n",
       "1        0.6875    0.6875         0.718182        0.6875  0.676113    0.6875  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./results/sentiment.tsv', delimiter='\\t')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
